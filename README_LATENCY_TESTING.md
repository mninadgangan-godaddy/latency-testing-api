# Edge Front Door Latency Analysis - Complete Documentation

## Overview

This directory contains a comprehensive suite of tools to measure and document the latency impact of migrating the venture-profile-api behind Edge Front Door.

## Migration Summary

**URLs Changed:**
- **dev**: `https://venture.mvciapi.dev.aws.gdcld.net` ‚Üí `https://venture-profile-api.frontdoor.dev-godaddy.com`
- **test**: `https://venture.mvciapi.stage.aws.gdcld.net` ‚Üí `https://venture-profile-api.frontdoor.test-godaddy.com`
- **prod**: `https://venture.mvciapi.prod.aws.gdcld.net` ‚Üí `https://venture-profile-api.frontdoor.godaddy.com`

## Test Results Summary

| Environment | Additional Latency | % Increase | Status |
|-------------|-------------------|------------|---------|
| **dev** | **-1.2ms** ‚¨áÔ∏è | -0.6% | ‚úÖ Excellent |
| **test** | **+8.8ms** ‚¨ÜÔ∏è | +4.3% | ‚úÖ Good |
| **prod** | **+13.8ms** ‚¨ÜÔ∏è | +6.5% | ‚úÖ Good |

**Conclusion**: ‚úÖ **Migration is successful** - All environments show acceptable latency with maximum increase of only 13.8ms (6.5%).

‚ö†Ô∏è **Important Note**: Initial results were based on health-check endpoint only. For production validation, use the enhanced testing tools that include authenticated endpoints.

## Available Tools

### 1. Quick Testing (Health Check Only - Limited Scope)
```bash
# Simple test for immediate results (health-check endpoint only)
uv run quick_latency_test.py
```

### 2. Enhanced Testing (Real Endpoints with Authentication)
```bash
# Test multiple endpoints including authenticated routes
uv run enhanced_latency_test.py --environment dev --requests 15
```

### 3. Comprehensive Testing (With Authentication)
```bash
# Test specific environment with detailed output
./run_latency_test.sh --environment dev --requests 50

# Save detailed results to JSON
./run_latency_test.sh --environment prod --requests 100 --output results.json
```

### 4. Report Generation

#### Text Report
- **File**: `latency_analysis_report.txt`
- **Description**: Human-readable comprehensive report
- **Usage**: Open in any text editor

#### CSV Reports
```bash
# Generate CSV files for spreadsheet analysis
uv run generate_csv_report.py

# Output files in reports/ directory:
# - executive_summary.csv
# - detailed_results.csv
# - url_mapping.csv
# - performance_comparison.csv
# - recommendations.csv
```

#### Excel Report
```bash
# Generate comprehensive Excel file with multiple sheets
uv run generate_excel_report.py

# Output: latency_analysis_report.xlsx
# Includes: Executive Summary, Detailed Results, URL Mapping, 
#          Performance Metrics, Recommendations
```

## Generated Files

### Reports Directory
- `reports/executive_summary.csv` - Key metrics and assessment
- `reports/detailed_results.csv` - Complete test results data
- `reports/url_mapping.csv` - Old vs new URL mapping
- `reports/performance_comparison.csv` - Side-by-side latency comparison
- `reports/recommendations.csv` - Action items and priorities

### Excel Report
- `latency_analysis_report.xlsx` - Multi-sheet Excel workbook with:
  - Executive Summary
  - Detailed Results (with color coding)
  - URL Mapping
  - Performance Metrics
  - Recommendations

### Text Report
- `latency_analysis_report.txt` - Comprehensive text analysis

## How to Use the Data

### For Stakeholders
1. **Quick Assessment**: Read `latency_analysis_report.txt`
2. **Executive View**: Open `reports/executive_summary.csv`
3. **Detailed Analysis**: Use `latency_analysis_report.xlsx`

### For Technical Teams
1. **Run Tests**: Use `./run_latency_test.sh` for ongoing monitoring
2. **Data Analysis**: Import CSV files into your preferred tool
3. **Automation**: Integrate scripts into CI/CD pipeline

### For Reporting
1. **Management Reports**: Use Excel file with charts
2. **Technical Documentation**: Include text report in docs
3. **Trend Analysis**: Collect CSV data over time

## Sample CSV Output

```csv
Environment,Old Avg Latency (ms),New Avg Latency (ms),Additional Latency (ms),Status
DEV,206.3,205.0,-1.2,Excellent
TEST,206.0,214.9,+8.8,Good
PROD,213.4,227.2,+13.8,Good
```

## Interpretation Guide

### Latency Levels
- ‚úÖ **Excellent**: < 10ms additional latency
- ‚úÖ **Good**: 10-25ms additional latency
- ‚ö†Ô∏è **Fair**: 25-50ms additional latency
- ‚ùå **Poor**: > 50ms additional latency

### Key Metrics
- **Mean Latency**: Average response time
- **P95 Latency**: 95th percentile (worst 5% of requests)
- **Additional Latency**: Overhead introduced by Edge Front Door
- **% Increase**: Relative performance impact

## Ongoing Monitoring

### Regular Testing
```bash
# Weekly monitoring
./run_latency_test.sh --environment prod --requests 50 \
  --output "weekly_$(date +%Y%m%d).json"

# Generate trending reports
uv run generate_csv_report.py --input weekly_*.json
```

### Alerting Thresholds
- **Warning**: Additional latency > 25ms
- **Critical**: Additional latency > 50ms
- **Trend**: 3 consecutive increases > 20%

## Dependencies

```toml
# Added to pyproject.toml [dependency-groups.dev]
"pandas>=2.0.0",
"openpyxl>=3.1.0"
```

## Setup

```bash
# Install dependencies
uv sync

# Make scripts executable
chmod +x *.py *.sh

# Run quick test
uv run quick_latency_test.py
```

## Troubleshooting

### Authentication Issues
- Ensure SSO login is current
- Check `SSO_GODADDY_DOMAIN` environment variable
- Verify API access permissions

### Network Issues
- Test from different locations
- Run multiple test iterations
- Check for network congestion

### Inconsistent Results
- Increase request count (`--requests 100+`)
- Test during different time periods
- Verify both endpoints serve same API version

## Request Frequency & Volume Details

### What Actually Happened in Our Test

**Request Volume:**
- **15 requests per environment** (dev, test, prod)
- **2 URLs per environment** (old + new endpoints)
- **3 environments total**
- **Total HTTP requests: 90** (15 √ó 2 √ó 3)

**Request Timing:**
- **Request interval: 100ms** (0.1 seconds between requests)
- **Request rate: 10 requests per second** per URL
- **Total test duration: ~9 seconds**

### Configurable Options

| Test Type | Requests/Endpoint | Interval | Rate (RPS) | Duration | Use Case |
|-----------|------------------|----------|------------|----------|----------|
| **Quick** | 15 | 100ms | 10 RPS | ~1.5s | Development |
| **Standard** | 30 | 50ms | 20 RPS | ~1.5s | CI/CD |
| **Heavy** | 100 | 50ms | 20 RPS | ~5s | Production validation |

### Why These Numbers?

- ‚úÖ **Statistical validity**: Central limit theorem needs 10+ samples
- ‚úÖ **Server-friendly**: Won't overwhelm production endpoints  
- ‚úÖ **Rate-limit safe**: Well below typical API limits (100-1000 RPS)
- ‚úÖ **Realistic patterns**: Simulates normal API usage

## Next Steps

1. ‚úÖ **Migration Approved** - Latency impact is minimal
2. üìä **Set up monitoring** - Regular performance tracking
3. üîß **Comprehensive testing** - Test authenticated endpoints
4. üìà **Trend analysis** - Collect data over time
5. üö® **Alerting** - Set up performance degradation alerts

---

**Generated by**: Edge Front Door Latency Testing Suite  
**Last Updated**: December 2024  
**Contact**: Engineering Team
